TODO file for FloPoCo
https://xkcd.com/844/
* List of operators to port to the new pipeline framework
	Remove an operator from the list below when it is done.
THIS INCLUDES WRITING ITS AUTOTEST

	IntConstMult: still need to 
	1/ add the delays 
	2/ resurrect IntConstMultRational (parseArgument etc)
	3 fix the mult by 7 in 2 additions 

	IntConstDiv: still need to
	1/ add the delays 
	2/ fix the bug that duplicates the tables

		Fix2FP
		FP2Fix
		InputIEEE
		OutputIEEE

		//FOR TEST PURPOSES ONLY
		Table

		IntComparator
		IntDualSub
		IntSquarer

		FPConstMult
		FPRealKCM
		FPConstDiv
		FixFunctionByTable
		FixFunctionBySimplePoly
		FixFunctionByPiecewisePoly
		BasicPolyApprox
		PiecewisePolyApprox
		FixRealKCM
		TestBench
		Wrapper
		FPAdd
		FPAddSub
		FPAddDualPath
		FPAdd3Input
		FPAddSinglePath
		FPMult
		//FPMultKaratsuba
		FPSquare
		FPDiv
		NbBitsMinRegisterFactory();
		FPSqrt

		FPLargeAcc
		LargeAccToFP
		FPDotProduct

		FPExp
		IterativeLog
		FPPow
		FixSinCos
		CordicSinCos
		FixAtan2
		//FixedComplexAdder

		FixRootRaisedCosine
		add FixSinc? 

		Complex/* (lots of cleanup)

		UserDefinedOperator
		
* Version 5 versus ASA book
** TODO before we submitt the book because of flopocoboxes
** TODO between the book submission and its publication
*** pipeline in IntAdder broken: fix discrepancy between prediction and adderDelay()
*** revive FixMultAdd so that plainVHDL is no longer a mandatory option
*** harmonize signedInput among the various FixFunctionBy

*** resurrect DualPortROM
*** fix-point-in, FP out version of FPExp 
** TODO some day
*** Resurrect and fuse CordicAtan2 and FixAtan2
*** resurrect DualTable
*** resurrect the low-latency adders
Problems: 
1/ we need a performance test framework
2/ it is probably quite xilinx-only at the moment
*** Check IntDualSub situation
*** resurrect Guillaume's work (IntPower etc)
*** FixSinCosPoly: use a signed reduced argument
*** generic Hsiao compression could improve most table-based algorithms
*** UniformPiecewisePolyApprox: we never try to give a0 more bits than the others. 
*** Class hierarchy for FixFunctionApprox
** FixSinCosPoly: use all the bit heaps that we advertize
** FixSinCosPoly:  see comments in FixSinCos.cpp One optim for 24 bits would be to compute zÂ² for free by a table using the second unused port of the blockram
* Small things in the pipeline framework
** constant signal scheduling TODO: check
The constant signals are currently all scheduled to cycle 0.
This is stupid: once the schedule is done (all the outputs of the top-level are scheduled).
they should be rescheduled ALAP (time zero, cyle= min of the successors)

** Signal::hasBeenScheduled_
Apart from the previous issue with constant signals, it seems we should remove most accesses to Signal::hasBeenScheduled_ that allow to re-schedule a signal:
   it should be initialized to false, then set once to true forever.
A quick grep seems to show it is the case.

** Re-check table attributes for 7 series; update Table, probably refactor this into Target 
** In the dot output, package shared components into dotted boxes (subgraph cluster_)
		It doesn't seem that simple
* Bugs
** constant 1000 bits in TestBench doesn't allow for parallel FFTs
** ./flopoco verbose=2 FixFunctionBySimplePoly plainvhdl=true f="sin(x)" lsbIn=-16 msbOut=4 lsbout=-16 TestBench n=-2
The parity problem leads to wrong alignment.
Nobody should do this if they have read the Muller book, so... people will try this and it is a bug
** ./flopoco verbose=2 FixFunctionBySimplePoly plainvhdl=true f="exp(x)" lsbIn=-16 msbOut=4 lsbout=-16 TestBench n=-2
	 
** ./flopoco plainVHDL=1 FixFunctionByPiecewisePoly f="(2^x-1)" d=2 lsbIn=-1 lsbOut=-8 msbout=0 testbench
** (check, it is an old bug) compression bug: ./flopoco IntMultiplier 2 16 16 1 0 0 does not produce a simple adder
* Wanted operators
** NormalCDF
... exists in the branch statistical_ops, old framework.
** FloatApprox
... exists in the random branch
** all in the random branch
** HOTBM
** Sum of n squares
** LUT-based integer comparators
** BoxMuller
* Current regressions:
** FPPipeline
** lut_rng
* Cleaning up
** Here and there, fix VHDL style issues needed for whimsical simulators or synthesizers. See doc/VHDLStyle.txt
** For Kentaro: avoid generating multiple times the same operators. 
** Doxygenize while it's not too late
** clean up Target
* Targets
** DONE Xilinx series 7
** Altera 10

* Towards continuous integration
** move to gitlab
** autotest at commit
** set up a performance regression test as well
* Improvements to do, operator by operator
** Collision
*** manage infinities etc
*** decompose into FPSumOf3Squares and Collision

** HOTBM
*** true FloPoCoization, pipeline
*** better (DSP-aware) architectural exploration

ConstMult:

** ConstMult
*** group KCM and shift-and-add in a single OO hierearchy (selecting the one with less hardware)
*** For FPConstMult, don't output the LSBs of the IntConstMult 
   but only their sticky
*** more clever, Lefevre-inspired algorithm
*** Use DSP: find the most interesting constant fitting on 18 bits
*** compare with Spiral.net and Gustafsson papers
*** Implement the continued fraction stuff for FPCRConstMult
		
** Shifters
*** provide finer spec, see the TODOs inside Shifter.cpp

General

* Janitoring
*** replace inPortMap and outPortMap by the modern interface newInstance()
		See FPAddSinglePath for examples
*** build a SNAP package https://docs.snapcraft.io/build-snaps/  
*** Add modern targets
*** replace the big ifs in Target.cpp with  method overloading in subtargets ? 
*** gradually convert everything to standard lib arithmetic, getting rid of the synopsis ones.
*** TargetFactory
*** rename pow2, intpow2 etc as exp2
*** doxygen: exclude unplugged operators
*** See table attributes above
*** remove Operator::signalList, replace it with signalMap altogether
(this must be considered carefully, we have several lists)
*** Replace pointers with smart pointers ?
* Bit heap and multipliers (old list, may be obsolete)
** DONE rewrite BitHeap with fixed-point support and better compression (see Kumm papers and uni_kassel branch)
** pipeline virtual IntMult
** See UGLYHACK in IntMultiplier
** IntSquarer should be made non-xilinx-specific, and bitheapized
** Same for IntKaratsuba and FPKaratsuba, which have been disabled completely
** Get rid of SignedIO in BitHeap: this is a multiplier concern, not a bit heap concern
** get rid of Operator::useNumericStd_Signed etc
** get rid of bitHeap::setSignedIO(signedIO);
** Check all these registered etc nonsense in Signal. Is it really used?
** Bug (ds FixRealKCM?) ./flopoco -verbose=3 FPExp 7 12 
** With Matei: see the nextCycles in FPExp and see if we can push them in IntMultiplier somehow

* BitHeapization (old list, may be obsolete) 
(and provide a bitheap-only constructor for all the following):
** systematic constructor interface with Signal variable
** Rework Guillaume Sergent's operators around the bit heap
** define a policy for enableSuperTile: default to false or true?
** Push this option to FPMult and other users of IntMult.
** Replace tiling exploration with cached/classical tilings
** More debogdanization: Get rid of
    IntAddition/IntCompressorTree
    IntAddition/NewIntCompressorTree
    IntAddition/PopCount
    after checking the new bit heap compression is at least as good...
** Check all the tests for "Virtex4"  src/IntAddSubCmp and replace them with tests for the corresponding features


Testbench

* Framework (old list, may be obsolete)
** Bug on outputs that are bits with isBus false and  multiple-valued  
  (see the P output of Collision in release 2.1.0)
** Multiple valued outputs should always be intervals, shouldn't they?
** global switch to ieee standard signed and unsigned libraries
** fix the default getCycleFromSignal . 


* Options for signed/unsigned  DONE, text should stay here while the janitoring isn't done
Option 0: Do nothing radical. It seems when the options
 --ieee=standard --ieee=synopsys
are passed to ghdl in this order, we may mix standard and synopsys entities
See directory TestsSigned  
Incrementally move towards option 1 (for new operators, and when needed on legacy ones)

Option 1: 
 * Keep only std_logic_vector as IO,
 * Add an option to declare() for signed / unsigned / std_logic_vector DONE
    The default should still be std_logic_vector because we don't want to edit all the existing operators
 * add conversions to the VHDL. DONE 
 * No need to edit the TestBench architecture (DONE, actually some editing was needed)

Option 2 (out: see discussion below)
 Same as Option 1, but allow signed/unsigned IOs
 * Need to edit the TestBench architecture
 * Cleaner but adds more coding. For instance, in Table, need to manage the types of IOs.
 - Too many operators have sign-agnostic information, e.g. Table and all its descendants

---------------------------------------------------
Should we allow signed/unsigned IO?
- Good reason for yes: it seems to be better (cleaner etc)
- Good reason for no: many operators don't care (IntAdder, all the Tables) 
  and we don't want to add noise to their interface if it brings no new functionality.
- Bad reason for no: it is several man-days of redesign of the framework, especially TestBench
  Plus several man-weeks to manually upgrade all the existing operators
Winner: NO, we keep IOs as std_logic_vector.

Should the default lib be standard (currently synopsys)?
Good reason for yes: it is the way forward
Bad reasons for no:  it requires minor editing of all existing operators 
Winner: YES, but after the transition to sollya4 is complete and we have a satisfiying regression test framework.


* DONE
** DONE  cleanup of the pipeline framework
2 use cases from Kassel:
 - we want to call optimal bit-heap scheduling algorithms, which will not be ASAP.
- we want to generate optimal adder DAGs, also not ASAP.
In both case, we want to provide to these algorithms the schedule of all the inputs.
  Typical case of the bit heap of a large multiplier: it adds 
     - bits from its DSP blocks (arrive after 2 or 3 cycles) 
     - bits from the logic-based multipliers (arrive at cycle 0 after a small delay)
  Real-world bit heaps (e.g. sin/cos or exp or log) have even more complex, difficult to predict BH structures.
1 use case from Lyon: pipelined adders (should know the schedule of the inputs to 

We want a robust solution that works for these use cases.
Current version 5 (hereafter refered to as Matei's code) is not efficient (it reschedules all the time) and overengineered WRT to these use cases.
Only drawback of the solution proposed below WRT Matei's code: it requires explicit calls to schedule() in some situations.
I consider this a good thing, it gives control.

Hypotheses:
H0: schedule is always called on the top-level operator.
  Even an explicit call to schedule() in a sub-component will schedule its top-level
  Beware: Wrapper and TestBench should not be parent operators of the operators they wrap, so as not to modify the schedule. 
H1: default schedule will always be ASAP. 
  A call to schedule() does what it can, then stops.
H2: schedule() does not reschedule anything: if a signal is already scheduled, it is skipped.
H3: shared operators are exclusively sub-cycle LUT-like operators (use cases so far: compressors, LUT-based mults in IntMultiplier)
  They define (possibly explicitely) the delay(s) between their input and output, but need not be scheduled. 

Schedule is called implicitely after the constructor of the top-level operator.
It may be called explicity by some code, in particular bit heap compression.
This somehow constraints the order of writing operator constructor code, but it is OK.
 
The algo should be:

If a bit heap bh is involved, the constructor
1/ perform all the bh->addBit(),
2/ explicitely calls schedule(),  which is supposed to schedule all the inputs 
	(this constrains constructor code order)
3/ calls generateCompressorVHDL(), which we delegate to Kassel.
Kassel compute their optimal architecture + schedule, and add it to the VHDL stream already scheduled
so that (thanks to H2) it will remain (and not be rescheduled ASAP)

For Martin: 
- Before generateCompressorVHDL is called, we will have the lexicographic timing 
  (i.e. cycle + delay within a cycle) for all the bits that are input to the bit heap.
  We really want Martin's algos to manage that.
  
- Martin's algorithms compute cycles + delays. Two options to exploit this information:
    a/ ignore the cycles, just have each signal declared with a delay in the compressor trees, 
       and hope the ASAP scheduler re-computes cycles that will  match those computed by Martin
    b/ let Martin directly hack the cycles and delays into the DAG -- probably much more code.
	I would vote for a/, but as Martin also minimizes registers, we should go for b/ 
To discuss.

- The BitHeap should be simplified, all the timing information should be removed: 
   it is now in the Signals (once they have been scheduled).
  So the actual interface to provide to Martin is not yet fixed.

** DONE Plan for bringup of the new pipeline framework
** DONE Shifter for basic pipeline: DONE
** DONE IntAdder for explicit call to schedule(): DONE
** DONE FPAdd for simple subcomponents : DONE
** DONE FPDiv for low complexity shared subcomponents DONE
** DONE FixRealKCM for simple bit heap DONE, 
** DONE FixSOPC DONE
** DONE FixFIR DONE
** DONE FixIIR for large bit heaps + functional delays: DONE
** DONE IntMult DONE
** DONE FixFunctionByTable (check that Table does the delay properly in the blockram case)
** DONE ALAP rescheduling for constant signals
** DONE FixSinCos for method=0
** DONE replace target->isPipelined() (and getTarget->isPipelined()) with isSequential()
Rationale: the two are redundant. isSequential is less prone to change during the life of an Operator... 
isSequential is properly initialized out of isPipelined in the default Operator constructor.
DONE more or less in Operator
** DONE Check that ?? and $$ and "port map" in comments don't ruin the pipeline framework
** DONE get rid of rst signal
Observation: no operator uses rst, except FixFIR and LargeAcc. 
There is a good reason for that: it would prevent the inference of srl logic.

Now FixFir doesn't manage rst in emulate(), which is a framework limitation.
LargeAcc ignores rst. Instead it has an additional newDataSet input, which technically induces a synchronous reset
We should generalize this way of expressing reset information.
Benefit: it will remove rst from all the classical pipelined operators, and explicit it only when it is useful.

** DONE get rid of use() in Operator
** DONE Get rid of the useBitHeap arg in KCMs
** DONE bug  ./flopoco FixSinCos -16 TestBenchFile 1000
   (close corresponding bug when fixed)
** DONE change interface to FixSinCos and CordicSinCos to use lsb and not w
** DONE IntConstMult: signed or unsigned int? (fix main.cpp)
** DONE rounding bug:  ./flopoco FixRealKCM 1 3 -10 -10 "Pi" 1 TestBenchFile 1000
  (close corresponding bug when fixed)
** DONE interface: simple and expert versions of IntMultiplier
** If we could start pipeline from scratch MOSTLY DONE
If we were to redo the pipeline framework from scratch, here is the proper way to do it.

The current situation has a history: we first added cycle management, then, as a refinement, critical-path based subcycle timing.
So we have to manage explicitely the two components of a lexicographic time (cycle and delay within a cycle)
But there is only one wallclock time, and the decomposition of this wallclock time into cycles and sub-cycles could be automatic. And should.
 
The following version of declare() could remove the need for manageCriticalPath as well as all the explicit synchronization methods.
declare(name, size, delay)
declares a signal, and associates its computation delay to it.  This delay is what we currently pass to manageCriticalPath. Each signal now will have a delay associated to it (with a default of 0 for signals that do not add to the critical path).
The semantics is: this signal will not be assigned its value before the instant delta + max(instants of the RHS signals) 
This is all what the first pass, the one that populates the vhdl stream, needs to do. No explicit synchronization management needed. No need to setCycle to "come back in time", etc.

Then we have a retiming procedure that must associate a cycle to each signal. 
It will do both synchronization and cycle computation. According to Alain Darte there is an old retiming paper that shows that the retiming problem can be solved optimally in linear time for DAGs, which is not surprising.
Example of simple procedure: 
first build the DAG of signals (all it takes is the same RHS parsing, looking for signal names, as we do)
Then sit on the existing scheduling literature...
For instance  
1/ build the operator's critical path
2/ build the ASAP and ALAP instants for each signal
3/ progress from output to input, allocating a cycle to each signal, with ALAP scheduling (should minimize register count for compressing operators)
4/ possibly do a bit of Leiserson and Saxe retiming 

We keep all the current advantages: 
- still VHDL printing based
- When developing an operator, we initially leave all the deltas to zero to debug the combinatorial version. Then we incrementally add deltas, just like we currently  add manageCriticalPath(). 
- etc

The difference is that the semantic is now much clearer. No more notion of a block following a manageCriticalPath(), etc

The question is: don't we loose some control on the circuit with this approach, compared to what we currently do?

Note that all this is so much closer to textbook literature, with simple DAGs labelled by delay...

Questions and remarks:
- what to do with setPipeline depth? Currently, it is set by hand, but the new framework allows for it to be computed automatically from the cycles of the circuit's outputs. What to do when the outputs are not synchronized?
- should it be allowed to have delayed signals in a port map?
- should the constant signals be actual signals?
- how to handle instances:
  - we should create a new class Instance, which contains a reference to the instanced Operator and a portMap for its inputs and outputs
  - Operator should have a flag isGlobal
  - Instance should have a flag isImplemented, signaling if the operator is on the global operator list and whether it has already been implemented, or not
  - Operator has a list of the instances it creates
  - Operator has a list of sub-operators
  - Target has the global operator list
  - when creating a new instance of a global operator
    - if it is the first, then just add it to the  global operator list, with the isImplemented flag to true
    - if it is not the first, then clone the existing operator, connecting the clone's inputs/outputs to the right signals, and set the isImplemented flag to true
  - the global operators exist in Target as well, and will be implemented there
  - there should be no cycles in the graph
  - all architectures are unrolled in the signal graph
  
  !- resource estimation during timing: we already have some information about the circuit's interal, so why not use this information for resource estimation, as well?


